# ğŸ“˜ String Readability Study â€” Replication Package
**Empirical Investigation to Understand the Impact of String Interpolation on Program Readability and Comprehension**

This repository contains all materials required to reproduce the quantitative and qualitative analyses reported in the study. The research investigates how **string interpolation** and **string concatenation** affect program readability, comprehension, and debugging efficiency among developers with prior programming experience.  
It includes datasets, analysis scripts, survey instruments, qualitative prompts, and replication instructions.

---

## ğŸ§  Overview
The study combines **quantitative task-based experiments** and **qualitative thematic analysis** to examine developer reasoning and performance differences when reading or debugging string expressions.

- **Participants:** 314
- **Design:** Mixed-method study with randomized task presentation
- **Tasks:** 16 programming questions comparing string concatenation and interpolation (four complexity levels)
- **Post-survey:** Five open-ended questions exploring readability, debugging, learning curve, preference, and improvement suggestions
- **Goal:** To empirically determine how syntactic style, familiarity, and code complexity influence readability and comprehension.

---

## ğŸ“ Repository Structure
```
string-readability-study/
string-readability-study/
â”œâ”€â”€ README.md                        # Main documentation
â”œâ”€â”€ LICENSE                          # Code/data license
â”œâ”€â”€ CITATION.cff                     # Citation metadata (for Zenodo DOI)
â”‚
â”œâ”€â”€ data/                            # Raw and processed datasets
â”‚   â”œâ”€â”€ raw/                         # Original anonymized responses
â”‚   â”œâ”€â”€ processed/                   # Cleaned and structured datasets
â”‚
â”œâ”€â”€ code/                            # Scripts and reproducibility assets
â”‚   â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ notebooks/                   # Jupyter notebooks for exploratory analysis
â”‚
â”œâ”€â”€ instruments/                     # Study instruments and materials
â”‚   â”œâ”€â”€ survey-app/                  # Web-based experiment platform (Flask)
â”‚   â””â”€â”€ llm/                         # LLM 
â”‚       â”œâ”€â”€ codebooks                # LLM qualitative codebooks
â”‚       â”œâ”€â”€ prompts                  # LLM qualitative prompts (RQ1â€“RQ5)
â”‚
â”œâ”€â”€ results/                         # Output and visualizations
â”‚   â”œâ”€â”€ figures/
â”‚   â”œâ”€â”€ csv_xlsx/

```

---

## âš™ï¸ Setup and Environment

### ğŸ”§ Prerequisites
- **Python** â‰¥ 3.10  
- **Pip** â‰¥ 23.0  
- **Git**, **Docker** (optional)

---

## â–¶ï¸ Reproduction Instructions
### Tool Setup
To replicate the experiment, you can simply follow the steps below to setup the data gathering tool.
#### Installation and Usage
1. **Running locally**
   - Setup the  by cloning the repository using `git clone` or download the repository.
   - It is a good practice to configure python virtual environment. Use the commands below to setup python virtual environment on `Linux/MacOS` or `Windows OS`. NB: The tool is designed using `Python 3.10` and tested on `Python 3.8` and `Python 3.9`.
   ```
   # For Linux/MacOS

   python3 -m venv venv
   source venv/bin/activate
   ```
   - For `Window OS` user, the easiest approach is to install `virtualenv` by running `pip install virtualenv`. The next step is pretty much similar to above;
   ```
   # For Window OS

   python3 -m virtualenv venv
   venv\Scripts\activate
   ```
   - Install required dependencies using `pip3 install -r requirements.txt`
   - Start the `experiment` by running `python3 app.py`. Ensure that port `5000` is open on your firewell. To interact with the experiment, go to your browser and type `localhost:5000` or `127.0.0.1:5000`.

2. **Running locally with "Docker for Desktop" - Not Yet Fully Tested!!**
   - Download and install `Docker fo Desktop` using the [link](https://www.docker.com/products/docker-desktop/). Once you are all set, run the commands below;
   ```
   git clone <project>
   cd /<project>/instruments/suvery-app
   docker compose up -d
   ```
  - That's all ğŸ˜!! the tool will be running on port `5000`. To interact with the experiment, go to your browser and type `localhost:5000`. You should be able to see the consent page of the experiment.
  
**NB: The ouput of the experiment will be stored in the [data](data) folder with the file name `responses.csv`**

---
### Issues 
Incase you encounter any challenge trying to reproduce this experiment, please feel free to report to the corresponding author of the paper.


Results appear in `results/figures/` and `results/csv_xlsx/`.

---

## ğŸ§© Qualitative Coding and Codebook
The repository includes:
- `instruments/prompts/` â€” standardized LLM prompts for RQ1â€“RQ5  
- `data/processed/codebook_final.csv` â€” validated codebook  
- `results/logs/qual_refinement.md` â€” coder agreement notes  

Coding followed the **Framework Method** with human validation of all model-assisted outputs.

---

## ğŸ“Š Summary of Key Results
- Interpolation improved readability and reduced comprehension time.  
- Familiarity influenced preferences, but interpolationâ€™s advantages were consistent across levels.  
- Tooling, syntax, and formatting were key readability factors.

---

## ğŸ§¾ License
- **Code:** MIT License  
- **Data:** Creative Commons Attribution 4.0 International (CC BY 4.0)
